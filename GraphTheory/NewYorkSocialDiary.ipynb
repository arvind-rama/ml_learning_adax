{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New York Social Diary\n",
    "\n",
    "[New York Social Diary](http://www.newyorksocialdiary.com/) casts a fascinating lens onto New York's socially well-to-do.  The data forms a natural social graph for New York's social elite.  Take a look at this page of a recent run-of-the-mill holiday party:\n",
    "\n",
    "`http://www.newyorksocialdiary.com/party-pictures/2014/holiday-dinners-and-doers`\n",
    "\n",
    "Besides the brand-name celebrities, you will notice the photos have carefully annotated captions labeling those that appear in the photos.  We can think of this as implicitly implying a social graph: there is a connection between two individuals if they appear in a picture together. In this project, we will scrape data from this website, parse the captions to find which people occur in photos together, and build a social graph of the result.\n",
    "\n",
    "The first step is to fetch the data.  This comes in two phases.\n",
    "\n",
    "The first step is to crawl the data.  We want photos from parties before December 1st, 2014.  Go to\n",
    "`http://www.newyorksocialdiary.com/party-pictures`\n",
    "to see a list of (party) pages.  For each party's page, grab all the captions.\n",
    "\n",
    "*Hints*:\n",
    "\n",
    "  1. Click on the on the index page and see how they change the url.  Use this to determine a strategy to get all the data.\n",
    "\n",
    "  2. Notice that each party has a date on the index page. You can use python's `datetime.strptime` function to parse it.\n",
    "\n",
    "  3. Some captions are not useful: they contain long narrative texts that explain the event.  Usually in two stage processes like this, it is better to keep more data in the first stage and then filter it out in the second stage.  This makes your work more reproducible.  It's usually faster to download more data than you need now than to have to redownload more data later.\n",
    "\n",
    "Now that you have a list of all captions, you should probably save the data on disk so that you can quickly retrieve it.  Now comes the parsing part.\n",
    "\n",
    "  1. Try to find some heuristic rules to separate captions that are a list of names from those that are not. For one, consider that long captions are often not lists of people.  The cutoff is subjective so to be definitive, *let's set that cutoff at 250 characters*.\n",
    "\n",
    "  2. You will want to separate the captions based on various forms of punctuation.  Try using `re.split`, which is more sophisticated than `string.split`.\n",
    "\n",
    "  3. You might find a person named \"ra Lebenthal\".  There is no one by this name.  Can anyone spot what's happening here?\n",
    "\n",
    "  4. This site is pretty formal and likes to say things like \"Mayor Michael Bloomberg\" after his election but \"Michael Bloomberg\" before his election.  Can you find other ('optional') titles that are being used?  They should probably be filtered out b/c they ultimately refer to the same person: \"Michael Bloomberg.\"\n",
    "\n",
    "For the analysis, we think of the problem in terms of a [network](http://en.wikipedia.org/wiki/Computer_network) or a [graph](http://en.wikipedia.org/wiki/Graph_%28mathematics%29).  Any time a pair of people appear in a photo together, that is considered a link.  What we have described is more appropriately called an (undirected) [multigraph](http://en.wikipedia.org/wiki/Multigraph) with no self-loops but this has an obvious analog in terms of an undirected [weighted graph](http://en.wikipedia.org/wiki/Graph_%28mathematics%29#Weighted_graph).  In this problem, we will analyze the social graph of the new york social elite.\n",
    "\n",
    "For this problem, we recommend using python's `networkx` library.\n",
    "\n",
    "\n",
    "## I. Degree\n",
    "\n",
    "The simplest question you might want to ask is 'who is the most popular'?  The easiest way to answer this question is to look at how many connections everyone has.  Write a function that returns the top 100 people and their degree.  Remember that if an edge of the graph has weight 2, it counts for 2 in the degree.\n",
    "  \n",
    "*Checkpoint:*\n",
    "\n",
    "    Top 100 .describe()\n",
    "    count    100.000000\n",
    "    mean     106.340000\n",
    "    std       51.509579\n",
    "    min       69.000000\n",
    "    25%       77.000000\n",
    "    50%       85.500000\n",
    "    75%      116.500000\n",
    "    max      372.000000\n",
    "\n",
    "\n",
    "## II. PageRank\n",
    "\n",
    " A similar way to determine popularity is to look at their [pagerank](http://en.wikipedia.org/wiki/PageRank).  Pagerank is used for web ranking and was originally [patented](http://patft.uspto.gov/netacgi/nph-Parser?patentnumber=6285999) by Google and is essentially the [stationary distribution](http://en.wikipedia.org/wiki/Markov_chain#Stationary_distribution_relation_to_eigenvectors_and_simplices) of a [markov chain](http://en.wikipedia.org/wiki/Markov_chain) implied by the social graph.\n",
    "\n",
    "Use 0.85 as the damping parameter so that there is a 15% chance of jumping to another vertex at random.\n",
    "\n",
    "*Checkpoint:*\n",
    "\n",
    "    Topp 100 .describe()\n",
    "    count    100.000000\n",
    "    mean       0.000185\n",
    "    std        0.000076\n",
    "    min        0.000124\n",
    "    25%        0.000138\n",
    "    50%        0.000162\n",
    "    75%        0.000200\n",
    "    max        0.000623\n",
    "   \n",
    "\n",
    "## III. Best Friends\n",
    "\n",
    "Another interesting question is who tend to co-occur with each other.  Give us the 100 edges with the highest weights. Write a function which returns a list of 100 tuples of the form ((person1, person2), count) in descending order of count\n",
    "\n",
    "    Topp 100 .describe()\n",
    "    count    100.000000\n",
    "    mean      25.070000\n",
    "    std       15.647154\n",
    "    min       13.000000\n",
    "    25%       15.000000\n",
    "    50%       19.000000\n",
    "    75%       28.500000\n",
    "    max      107.000000\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%xmode Context\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "import requests\n",
    "from collections import namedtuple\n",
    "from datetime import datetime \n",
    "PartyPictures = namedtuple('PartyPictures', 'title, url, date,html')\n",
    "PartyPicturesDetails = namedtuple('PartyPictures','imgurl,text')\n",
    "\n",
    "def get_party_pictures(fromdatetime=datetime(2015, 11, 30)):\n",
    "    party_pictures=[]\n",
    "    for i in range(2,29):\n",
    "        response = requests.get(\"http://www.newyorksocialdiary.com/party-pictures\", params={\"page\": str(i)})\n",
    "        soup = BeautifulSoup(response.text, \"lxml\")\n",
    "        parent_div = soup.find_all(\"div\", class_=\"view-content\")\n",
    "        rows = parent_div[0].find_all(\"div\",class_=\"views-row\")\n",
    "        for row in rows:\n",
    "            url = row.select(\"span.views-field-title\")[0].select(\"span.field-content\")[0].select(\"a\")[0].get('href')\n",
    "            title = row.select(\"span.views-field-title\")[0].select(\"span.field-content\")[0].select(\"a\")[0].getText()\n",
    "            date = row.select(\"span.views-field-created\")[0].select(\"span.field-content\")[0].getText()\n",
    "            formatteddate = datetime.strptime(date, '%A, %B %d, %Y')\n",
    "            if (formatteddate < fromdatetime):\n",
    "                party_pictures.append(PartyPictures(url=url,title=title,date=date,html=''))\n",
    "    return party_pictures\n",
    "\n",
    "import pickle\n",
    "\n",
    "def get_party_picture_html(response,party_pic):\n",
    "    return PartyPictures(url=party_pic.url,title=party_pic.title,date=party_pic.date,html=response.text)\n",
    "    \n",
    "\n",
    "from requests_futures.sessions import FuturesSession\n",
    "def get_people_party_pictures_details():\n",
    "    party_pictures_details = []\n",
    "    url_base = \"http://www.newyorksocialdiary.com\"\n",
    "    session = FuturesSession(max_workers=15)\n",
    "    party_pics = get_party_pictures()\n",
    "    futures = [session.get(urljoin(url_base, party_pic.url)) for party_pic in party_pics]\n",
    "    party_pictures_details = [get_party_picture_html(future.result(), party_pic) for future, party_pic in zip(futures, party_pics)]\n",
    "    pickle.dump( party_pictures_details, open( \"party_picture_details.p\", \"wb\" ) )\n",
    "    return party_pictures_details\n",
    "\n",
    "\n",
    "def get_images_party_pictures(party_pictures):\n",
    "    my_dict = {}\n",
    "    for party_picture in party_pictures:\n",
    "        party_images = []\n",
    "        soup = BeautifulSoup(party_picture.html, \"lxml\")\n",
    "        images = soup.find_all(\"img\")\n",
    "        for image in images:\n",
    "            tables = image.find_all_previous(\"table\")\n",
    "            if (len(tables)) > 0:\n",
    "                photocaption = tables[0].select(\".photocaption\")\n",
    "                if (len(photocaption)) > 0:\n",
    "                    text = photocaption[0].getText()\n",
    "                    if (\"for NYSD Contents\" not in text):\n",
    "                        party_images.append(PartyPicturesDetails(imgurl=image.get('src'),text=text))\n",
    "                else:\n",
    "                    font = tables[0].select(\"font\")\n",
    "                    if (len(font)) > 0:\n",
    "                        text = font[0].getText()\n",
    "                        if (\"for NYSD Contents\" not in text):\n",
    "                            party_images.append(PartyPicturesDetails(imgurl=image.get('src'),text=text))\n",
    "        my_dict[party_picture.url] = party_images \n",
    "    return my_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "party_pictures_details_list = get_people_party_pictures_details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "party_pictures_details_list = pickle.load( open( \"party_picture_details.p\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_dict = get_images_party_pictures(party_pictures_details_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def get_network_for_party_pictures(partypictures_list_of_lists):\n",
    "    my_network = {}\n",
    "    #all_people_list = []\n",
    "    for partypicture_list in partypictures_list_of_lists:\n",
    "        for partypicture in partypicture_list:\n",
    "            finallist = []\n",
    "            excludes = ['Click','Dinner','Violet Ball', 'Steering Committee','Girl Scouts', 'Future Woman',\n",
    "                        'Chicago Botanic Garden Summer Gala','Sending','Birthday Party',\n",
    "                        'Hudson Terrace','The Plaza Hotel Grand Ballroom','The','Media Award',\n",
    "                        'Interm President','Lobel Modern', 'Century Gallery','Guests','Fountain',\n",
    "                        'Los Angeles','East Hampton Village','Dinner','Villa Pisani','Glow Zone',\n",
    "                        'Marlborough','Hospital','Museum','Foundation','Clockwise','Trustee',\n",
    "                        'Special Surgery','Honoree','Chicago','M.D','Jr','Yugoslavia','New York',\n",
    "                        'The Society','Guest','Trustees','Members', \"York\", 'L','Directors', 'Universe',\n",
    "                        'Dancing', 'Arriving', 'African Mammals','Cocktails','The Metropolitan Club',\n",
    "                        'Dressed','Keynote','MD','PhD','PsyB','The','The Society','Memorial Sloan Kettering',\n",
    "                        'The Four Seasons Restaurant','Committee','Jay Heritage Center','CEO NCR','Ceremonies',\n",
    "                        'NY State Assemblyman','Children Award Executive Director','Governors Award',\n",
    "                        'Children Award Board','Directors','The Soul Rebels','Island Weiss','YAGP',\n",
    "                        'American Ballet Theatre','Brooklyn Museum','Ocean Life','Natural History',\n",
    "                        'Mayor','Governor','Click','Contents','Miss','Dr.','Medical',\n",
    "                        'Langone','Countess','Photographs','Distinction', 'Ambassador',\n",
    "                        'Councilwoman','Annual', 'Designer', 'Showhouse',\n",
    "                        'Cocktails', 'Esplanade', 'Publicists','Benefit', 'Mstr', 'Sgt',\n",
    "                        'Presented','Choreographer','Actor','Congresswoman','County',\n",
    "                        'Legislator','Honorary','Gala','Chairman','Co-Chairmen','Board','Member',\n",
    "                        'Executive','Director','Chair','Co-Chairs','Gallery','Artist','Event',\n",
    "                        'President','Steering', 'Committee','Duchess','Jr','Vice','Frick',\n",
    "                        'New', 'Art', 'Co', 'Front','(Front)', 'Back','(Back)','Center','Former', 'Author','Deputy',\n",
    "                        'Curator', 'Honorees', 'Chief', 'Serbia',\n",
    "                        'Museum Gala Chairs','Museum Gala Chair','A','The MAD',\"Young Attending Award\",\"CEO\",\"M\",\"Mr\"\n",
    "                        'American Ballet','School','Miss USA','Miss Teen USA',\"H\"]\n",
    "\n",
    "            if (len(partypicture.text) >= 250):\n",
    "                continue\n",
    "            text = partypicture.text.strip()\n",
    "            #print(text)\n",
    "            \n",
    "            #Remove non name capitalized words which are part of names\n",
    "            text = re.sub(r'(\\(Trey\\))|Brooklyn DA|(\\(Museum\\s+Trustee\\))|(Museum\\s+Gala\\s+Chair)|(Honorable\\s+)|(Trustees\\s+)|(Trustee\\s+)|(Honorees\\s+)|(Honoree\\s+)|(JHC\\s+President\\s+)|(Museum\\s+Trustee\\s+)|(Former\\s+MA\\s+)|(Former\\s+DA\\s+)|(Museum\\s+President\\s+)|(Co-Chair\\s+)|','',text)\n",
    "            \n",
    "            #Remove extra spaces and replace with one space\n",
    "            text = re.sub(r'\\s{2,}',' ',text)\n",
    "            \n",
    "            #print(text)\n",
    "            \n",
    "            # Deal with sentences like \"Mr. and Mrs.Will Smith as just Will Smith\"\n",
    "            searchresult = re.search(r'Mr(?:.)\\s+and\\sMrs(?:.)((\\s)?([A-z]\\w*)(\\s[A-z]\\w*)?)',text.strip())\n",
    "            if searchresult:\n",
    "                text = searchresult.group(1).strip()\n",
    "            \n",
    "            # Deal with sentences like  Will and Jada Smith as Will Smith and Jada Smith at start of sentence\n",
    "            replacesearchresult = re.search(r'^[A-Z]{1}\\w*\\s+and\\s+[A-Z]{1}\\w*\\s+([A-Z]{1}\\w*(\\s+[A-Z]{1}\\w*)?)(,\\s+Jr.{0,1})?',text)\n",
    "            if replacesearchresult:\n",
    "                text = re.sub(r' and', \" \"+replacesearchresult.group(1)+\" and\", text)\n",
    "            \n",
    "            #Split the sentence\n",
    "            wordlist = re.split(': |,\\sand |, |\\sand |\\swith |\\sof |\\sat',text)\n",
    "            \n",
    "            \n",
    "            #Finding the names\n",
    "            for newwords in wordlist:\n",
    "                searchresult = re.search(r\"^(([A-Z]{1}\\w*\\.\\s+)?([A-Z]{1}\\w*)(\\s+du)?(\\s+van)?(\\s+von)?(\\s+del)?(\\s+van\\s+der)?(\\s+de\\s+la)?(\\s+de)?(\\s+[A-Z]([a-z])?\\.)?(\\s+[A-Z]{1}\\w*|(-\\w*))?('[A-Z]{1}\\w*)?(\\s+[A-Z]{1}\\w*)?(,\\s+Ph.D)?)|([A-Z]{1}\\w*\\.\\s+)?([A-Z]{1}\\w*)(\\s+du)?(\\s+van)?(\\s+von)?(\\s+del)?(\\s+van\\s+der)?(\\s+de\\s+la)?(\\s+de)?(\\s+[A-Z]([a-z])?\\.)?(\\s+[A-Z]{1}\\w*|(-\\w*))?('[A-Z]{1}\\w*)?(\\s+[A-Z]{1}\\w*)?(,\\s+Ph.D)?\\Z\",newwords.strip())\n",
    "                if searchresult:\n",
    "                    if (searchresult.group().strip() in excludes):\n",
    "                         continue\n",
    "                    else:\n",
    "                        finallist.append(searchresult.group().strip())\n",
    "            #print(finallist)            \n",
    "            #Dict with person --> friends\n",
    "            if (len(finallist) > 1):\n",
    "                for name in finallist:\n",
    "                    if name not in my_network:\n",
    "                        templist = list(finallist)\n",
    "                        if (len(templist) > 1):\n",
    "                            templist.remove(name)\n",
    "                            my_network[name] = list(templist)\n",
    "                        else:\n",
    "                            my_network[name] = []\n",
    "                    else:\n",
    "                        templist = list(finallist)\n",
    "                        if (len(templist) > 1):\n",
    "                            templist.remove(name)\n",
    "                            my_network[name] =  list(my_network[name]) + list(templist)\n",
    "    return my_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "my_network_dict = get_network_for_party_pictures(list(my_dict.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "G = nx.Graph()\n",
    "for node in list(my_network_dict.keys()):\n",
    "    for each_node in my_network_dict[node]:\n",
    "        edge_dict = G.get_edge_data(node,each_node)\n",
    "        if (edge_dict is None):\n",
    "            G.add_edge(node,each_node,weight=1)\n",
    "        else:\n",
    "            G.add_edge(node,each_node,weight=(edge_dict[\"weight\"]+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Degree, Degree Weighted and Page Rank\n",
    "import pandas as pd\n",
    "names = []\n",
    "degree = []\n",
    "degrees = G.degree(weight=1)\n",
    "for key,val in list(degrees.items()):\n",
    "    names.append(key)\n",
    "    degree.append(val)\n",
    "my_df = pd.DataFrame(data = list(zip(names,degree)), columns=['Name', 'Degree'])\n",
    "my_df.set_index(\"Name\")\n",
    "\n",
    "names = []\n",
    "degree = []\n",
    "degrees = G.degree(weight=\"weight\")\n",
    "for key,val in list(degrees.items()):\n",
    "    names.append(key)\n",
    "    degree.append(val)\n",
    "my_df2 = pd.DataFrame(data = list(zip(names,degree)), columns=['Name', 'DegreeWeighted'])\n",
    "my_df2.set_index(\"Name\")\n",
    "\n",
    "names = []\n",
    "pageranklist = []\n",
    "pageranks = nx.pagerank(G,alpha=0.85)\n",
    "for key,val in list(pageranks.items()):\n",
    "    names.append(key)\n",
    "    pageranklist.append(val)\n",
    "my_df3 = pd.DataFrame(data = list(zip(names,pageranklist)), columns=['Name', 'Pagerank'])\n",
    "my_df3.set_index(\"Name\")\n",
    "\n",
    "my_df_full = my_df.merge(my_df2).merge(my_df3)\n",
    "my_df_full = my_df_full.sort(columns=[\"Degree\"],ascending=False)\n",
    "\n",
    "my_df_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_friends_tuple():\n",
    "    my_tuple_list = []\n",
    "    pair_dict = {}\n",
    "    for each_tuple in G.nodes(data=True):\n",
    "        myfriendsset = set(G[each_tuple[0]])\n",
    "        for friend in myfriendsset:\n",
    "            edge_dict = G.get_edge_data(each_tuple[0],friend)\n",
    "            #prevent duplicates\n",
    "            if friend+\"_\"+each_tuple[0] not in pair_dict:\n",
    "                my_tuple_list.append(((each_tuple[0],friend),edge_dict[\"weight\"]))\n",
    "                #Add pair to dict for faster traversal to find duplicates\n",
    "                pair_dict[each_tuple[0]+\"_\"+friend] = True \n",
    "    my_tuple_list.sort(key=lambda tup: tup[1],reverse=True)\n",
    "    return my_tuple_list\n",
    "\n",
    "my_friends_tuple()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
