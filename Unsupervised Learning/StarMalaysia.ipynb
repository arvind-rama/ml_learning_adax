{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Star Malaysia Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the data. Comment out the following line after running it once.\n",
    "!wget 139.59.226.45:8000/full.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the data into a pandas DataFrame\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('full.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Keyword extraction\n",
    "\n",
    "Try to extract some relevant words for each article. Count the number of occurences of each word in each document. Read more on scikit-learn about how to extract features from text: http://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction\n",
    "\n",
    "1. Use [CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)\n",
    "1. Use [TfIdfVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)\n",
    "\n",
    "See how you can handle stop words:\n",
    "- with external resources\n",
    "- automatically based on the corpus frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  Keyword extraction\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "countVect = CountVectorizer(stop_words=nltk.corpus.stopwords.words('english'))\n",
    "countfit = countVect.fit(df[\"text\"].replace(np.nan, '', regex=True).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "counttransform = countVect.transform(df[\"text\"].replace(np.nan, '', regex=True).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "x = countVect.vocabulary_\n",
    "sorted_x = sorted(x.items(), key=operator.itemgetter(1),reverse=True)\n",
    "sorted_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "\n",
    "df[\"text\"] = df[\"text\"].replace(np.nan, '', regex=True)\n",
    "tfidvect = TfidfVectorizer(stop_words=nltk.corpus.stopwords.words('english'))\n",
    "fittfidvect = tfidvect.fit(df[\"text\"])\n",
    "transform_text_df = tfidvect.transform(df[\"text\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fittfidvect?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Find similar articles\n",
    "\n",
    "The goal of this section is to propose similar articles to the reader. It might be something similar to the section \"You May Be Interested\" when you're reading one publication on http://www.thestar.com.my/.\n",
    "\n",
    "Let's do it finding the most closest articles to each article.\n",
    "\n",
    "We have to define distance/similary between two documents.\n",
    "1. Try with Euclidian distance\n",
    "1. Try with [Cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity)\n",
    "\n",
    "Hint: [Nearest Neighbors from scikit-learn](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.NearestNeighbors.html#sklearn.neighbors.NearestNeighbors) \n",
    "\n",
    "Going further: this is an unsupervised algorithm. Which data would you need to compare the two apporache (Euclidian vs Cosine)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_text_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Nearest neighbors\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "nneighbors = NearestNeighbors()\n",
    "nneighbors.fit(transform_text_df)\n",
    "distances, indices = nneighbors.kneighbors(transform_text_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Clustering\n",
    "\n",
    "Assume that cosine similarity works better on text documents. Choose a clustering algorithm is order to group documents that are about the same subject. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans() \n",
    "\n",
    "# Now we perform the clustering\n",
    "kmeans.fit(transform_text_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 - NMF\n",
    "\n",
    "Let's use the NMF technique to mine the dataset further.\n",
    "\n",
    "1. You will first need to transform the text into a matrix using `TfidfVectorizer()` ([docs](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)). Be sure to use the stopwords. Some of the articles are blank and so their content comes up as NaN. Use the function `DataFrame.notnull()` to eliminate these from your sample.\n",
    "2. Use the `NMF` estimator to divide the dataset into 10 topics.\n",
    "3. Print the top 20 words corresponding to each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NMF\n",
    "\n",
    "from sklearn.decomposition import NMF\n",
    "n_topics = 10\n",
    "n_top_words = 20\n",
    "\n",
    "nmf = NMF(n_components=n_topics, random_state=1).fit(transform_text_df)\n",
    "\n",
    "feature_names = tfidvect.get_feature_names()\n",
    "\n",
    "for topic_idx, topic in enumerate(nmf.components_):\n",
    "    print((\"Topic #%d:\" % topic_idx))\n",
    "    print((\" \".join([feature_names[i]\n",
    "                    for i in topic.argsort()[:-n_top_words - 1:-1]])))\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
