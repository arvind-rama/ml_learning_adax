{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def load_yelp_review(path='yelp_academic_dataset_review.json'):\n",
    "    reviews = {}\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            jsonline = json.loads(line)\n",
    "            reviews[jsonline['text']] = jsonline['stars']\n",
    "    return reviews\n",
    "\n",
    "def load_yelp_business(path='yelp_academic_dataset_business.json'):\n",
    "    business_stars = {}\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            jsonline = json.loads(line)\n",
    "            business_id = jsonline['business_id']\n",
    "            business_stars[business_id] = jsonline['stars']\n",
    "    return business_stars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reviews_stars = load_yelp_review()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "business_stars = load_yelp_business()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install rarfile\n",
    "!pip install unrar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#http://www.developintelligence.com/blog/2017/03/predicting-yelp-star-ratings-review-text-python/\n",
    "from collections import Counter\n",
    "def balance_classes(xs, ys):\n",
    "    freqs = Counter(ys)\n",
    "    max_allowable = freqs.most_common()[-1][1]\n",
    "    num_added = {clss: 0 for clss in freqs.keys()}\n",
    "    new_ys = []\n",
    "    new_xs = []\n",
    "    for i, y in enumerate(ys):\n",
    "        if num_added[y] < max_allowable:\n",
    "            new_ys.append(y)\n",
    "            new_xs.append(xs[i])\n",
    "            num_added[y] += 1\n",
    "    return new_xs, new_ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#reduce size because of memory issues\n",
    "balanced_x, balanced_y = balance_classes(list(reviews_stars.keys())[:10000], list(reviews_stars.values())[:10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(balanced_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidVectorizer = TfidfVectorizer(ngram_range=(1,2) ,stop_words=nltk.corpus.stopwords.words('english'))\n",
    "vectors = tfidVectorizer.fit_transform(balanced_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#prediction\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(vectors, balanced_y, test_size=0.33, random_state=42)\n",
    "\n",
    "# initialise the SVM classifier\n",
    "classifier = LinearSVC()\n",
    "classifier.fit(X_train, y_train)\n",
    "preds = classifier.predict(X_test)\n",
    "svc_score = accuracy_score(y_test, preds)\n",
    "\n",
    "multinomialNBclassifier = MultinomialNB(alpha=0.3)\n",
    "multinomialNBclassifier.fit(X_train, y_train)\n",
    "preds = multinomialNBclassifier.predict(X_test)\n",
    "\n",
    "nb_score = accuracy_score(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Stemming\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "def stem_tokens(tokens, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stems = stem_tokens(tokens, stemmer)\n",
    "    return stems\n",
    "\n",
    "tfidVectorizerstemmed = TfidfVectorizer(ngram_range=(1,2),tokenizer=tokenize ,stop_words=nltk.corpus.stopwords.words('english'))\n",
    "vectorsstemmed = tfidVectorizerstemmed.fit_transform(balanced_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(vectorsstemmed, balanced_y, test_size=0.33, random_state=42)\n",
    "\n",
    "multinomialNBclassifier = MultinomialNB(alpha=0.3)\n",
    "multinomialNBclassifier.fit(X_train, y_train)\n",
    "preds = multinomialNBclassifier.predict(X_test)\n",
    "\n",
    "nb_score_stemmed = accuracy_score(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Pos Tagging using adjectives\n",
    "def pos_tokens(tokens):\n",
    "    tagged_items = nltk.pos_tag(tokens)\n",
    "    adjective_tokens = []\n",
    "    for item in tagged_items:\n",
    "        if item[1] == 'JJ':\n",
    "            adjective_tokens.append(item[0])\n",
    "    return adjective_tokens\n",
    "\n",
    "def pos_tokenize(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    adjective_tokens = pos_tokens(tokens)\n",
    "    return adjective_tokens\n",
    "\n",
    "tfidVectorizeradj = TfidfVectorizer(ngram_range=(1,2),tokenizer=pos_tokenize ,stop_words=nltk.corpus.stopwords.words('english'))\n",
    "vectorsadj = tfidVectorizeradj.fit_transform(balanced_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(vectorsadj, balanced_y, test_size=0.33, random_state=42)\n",
    "\n",
    "multinomialNBclassifier = MultinomialNB(alpha=0.3)\n",
    "multinomialNBclassifier.fit(X_train, y_train)\n",
    "preds = multinomialNBclassifier.predict(X_test)\n",
    "\n",
    "nb_score_pos = accuracy_score(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#install unrar on o/s: sudo apt install unrar\n",
    "dl_url = 'http://www.cs.uic.edu/~liub/FBS/opinion-lexicon-English.rar'\n",
    "local_name = 'opinion-lexicon-English.rar'\n",
    "\n",
    "# Set to True to download the .rar archive\n",
    "if (True):    \n",
    "    import urllib.request, urllib.parse, urllib.error\n",
    "    testfile = urllib.request.FancyURLopener()\n",
    "    testfile.retrieve(dl_url, local_name)\n",
    "\n",
    "# Set to True to extract files from the archive\n",
    "if (True):\n",
    "    import rarfile\n",
    "    rar = rarfile.RarFile(local_name)\n",
    "    rar.printdir()\n",
    "    rar.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Sentiment using positive and negative words\n",
    "def read_wordlist(fname):\n",
    "    with open(fname,encoding='latin1') as f:\n",
    "        wordlist = [line.strip() for line in f.readlines() \n",
    "                    if (not line[0]==';') and line.strip()\n",
    "                   ]\n",
    "    worddict = dict((word,True) for word in wordlist)\n",
    "    return worddict\n",
    "\n",
    "positive_words = read_wordlist('positive-words.txt')\n",
    "negative_words = read_wordlist('negative-words.txt')\n",
    "\n",
    "def senti_tokens(tokens):\n",
    "    sentiment_tokens = []\n",
    "    for item in tokens:\n",
    "        if (item in positive_words or item in negative_words):\n",
    "            sentiment_tokens.append(item)\n",
    "    return sentiment_tokens\n",
    "\n",
    "def senti_tokenize(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    sentiment_tokens = senti_tokens(tokens)\n",
    "    return sentiment_tokens\n",
    "\n",
    "tfidVectorizersenti = TfidfVectorizer(ngram_range=(1,2),tokenizer=senti_tokenize ,stop_words=nltk.corpus.stopwords.words('english'))\n",
    "vectorssenti = tfidVectorizersenti.fit_transform(balanced_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(vectorssenti, balanced_y, test_size=0.33, random_state=42)\n",
    "\n",
    "multinomialNBclassifier = MultinomialNB()\n",
    "multinomialNBclassifier.fit(X_train, y_train)\n",
    "preds = multinomialNBclassifier.predict(X_test)\n",
    "\n",
    "nb_score_senti = accuracy_score(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def combined_pos_senti_tokens(tokens):\n",
    "    new_tokens = []\n",
    "    tagged_items = nltk.pos_tag(tokens)\n",
    "    for item in tagged_items:\n",
    "        if item[1] == 'JJ':\n",
    "            new_tokens.append(item[0])\n",
    "        elif (item[0] in positive_words or item[0] in negative_words):\n",
    "            new_tokens.append(item[0])\n",
    "    return new_tokens\n",
    "\n",
    "def combined_pos_senti_tokenize(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    new_tokens = combined_pos_senti_tokens(tokens)\n",
    "    return new_tokens\n",
    "\n",
    "tfidVectorizercombo = TfidfVectorizer(ngram_range=(1,2),tokenizer=combined_pos_senti_tokenize ,stop_words=nltk.corpus.stopwords.words('english'))\n",
    "vectorscombo = tfidVectorizercombo.fit_transform(balanced_x)\n",
    "X_train, X_test, y_train, y_test = train_test_split(vectorscombo, balanced_y, test_size=0.33, random_state=42)\n",
    "\n",
    "multinomialNBclassifier = MultinomialNB()\n",
    "multinomialNBclassifier.fit(X_train, y_train)\n",
    "preds = multinomialNBclassifier.predict(X_test)\n",
    "\n",
    "nb_score_combo = accuracy_score(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "D = {u'Multinomial_NB':nb_score*100, u'SVC': svc_score*100, u'Positive_Neg': nb_score_senti*100 \n",
    "     , u'Stemmed': nb_score_stemmed*100, u'Adjective_POS':nb_score_pos*100 ,\n",
    "    u'Adjective_Senti':nb_score_combo*100}\n",
    "plt.figure(figsize=(12,9))\n",
    "plt.bar(range(len(D)), D.values(), align='center')\n",
    "plt.xticks(range(len(D)), D.keys())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_yelp_business_review(path='yelp_academic_dataset_review.json'):\n",
    "    business_reviews = {}\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            jsonline = json.loads(line)\n",
    "            business_id = jsonline['business_id']\n",
    "            if business_id in business_reviews.keys():\n",
    "                new_list = business_reviews[business_id]\n",
    "                new_list.append(jsonline['text'])\n",
    "                business_reviews[business_id] = new_list\n",
    "            else:\n",
    "                business_reviews[business_id] = [jsonline['text']]\n",
    "    return business_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "business_reviews = load_yelp_business_review()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#check for 100 businesses\n",
    "limit = 100\n",
    "count = 0\n",
    "preds = {}\n",
    "for key in business_reviews.keys():\n",
    "    count = count + 1\n",
    "    total_reviews = 0\n",
    "    score = 0\n",
    "    reviews = business_reviews[key]\n",
    "    for review in reviews:\n",
    "        total_reviews = total_reviews + 1\n",
    "        score =score + classifier.predict(tfidVectorizer.transform([review]))\n",
    "    preds[key] = score/total_reviews\n",
    "    if count == 100:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predictions for 100 Businesses\n",
    "for key in preds.keys():\n",
    "    print('{} actual :{} predicted: {}'.format(key, business_stars[key] , preds[key][0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
