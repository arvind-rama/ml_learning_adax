{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "Our objective is to predict a new venue's popularity from information available when the venue opens.  We will do this by machine learning from a dataset of venue popularities provided by Yelp.  The dataset contains meta data about the venue (where it is located, the type of food served, etc ...).  It also contains a star rating.  \n",
    "\n",
    "For this project, the star rating will be our **dependent variable.** All of the other information in the dataset will be our **independent variables.** \n",
    "\n",
    "There are 5 parts to this project. In parts 1 through 4, you will build estimators which each use different independent variables in order to predict the dependent variable. In part 5, you will combine the estimators you have created in parts 1 through 4 to build a FullModel, which performs better than any of its components parts alone.\n",
    "\n",
    "## Download and parse the data\n",
    "\n",
    "In your VM, use the `wget` command to download the dataset:\n",
    "\n",
    "```bash\n",
    "wget http://eds.thecads.org/yelp_train_academic_dataset_business.json.gz\n",
    "```\n",
    "\n",
    "Notice that each row of the file is a json blurb.  You can read it in python.  *Hints:*\n",
    "1. `gzip.open` ([docs](https://docs.python.org/2/library/gzip.html)) has the same interface as `open` but is for `.gz` files.\n",
    "2. The library `json` has a function called `loads()` which can read a single json blurb into a Python dictionary\n",
    "\n",
    "Put the data into a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Imports for project\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gzip\n",
    "import json\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "import sklearn as sk\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load up the data\n",
    "with gzip.open('yelp_train_academic_dataset_business.json.gz', \"rb\") as f:\n",
    "    yelp_data = []\n",
    "    for i in f:\n",
    "        yelp_data.append(json.loads(i)) \n",
    "\n",
    "source_df = pd.DataFrame(yelp_data)\n",
    "source_df.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup cross-validation:\n",
    "In order to track the performance of your machine-learning, use `cross_validation.train_test_split` ([docs](http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.train_test_split.html)).\n",
    "\n",
    "Be sure to also separate the dependent and independent variables at this stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot relationships\n",
    "y = source_df[\"stars\"]\n",
    "for col in source_df.columns:\n",
    "    if (col != \"stars\" and source_df[col].dtypes == np.float64 or source_df[col].dtypes == np.int64  ):\n",
    "        plt.figure()\n",
    "        plt.title(col)\n",
    "        plt.plot(source_df[col], y, '.')\n",
    "        plt.xlabel(col)\n",
    "        plt.ylabel('Stars')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
    "y = source_df[\"stars\"]\n",
    "X = source_df.ix[:, source_df.columns != 'stars']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train the model using the training sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building models in sklearn\n",
    "\n",
    "All estimators (e.g. linear regression, kmeans, etc ...) support `fit` and `predict` methods.  In fact, you can build your own by inheriting from classes in `sklearn.base` by using this template:\n",
    "``` python\n",
    "class Estimator(base.BaseEstimator, base.RegressorMixin):\n",
    "  def __init__(self, ...):\n",
    "   # initialization code\n",
    "\n",
    "  def fit(self, X, y):\n",
    "    # fit the model ...\n",
    "    return self\n",
    "\n",
    "  def predict(self, X):\n",
    "    return # prediction\n",
    "```\n",
    "The intended usage is:\n",
    "``` python\n",
    "estimator = Estimator(...)  # initialize\n",
    "estimator.fit(X_train, y_train)  # fit data\n",
    "y_pred = estimator.predict(X_test)  # predict answer\n",
    "estimator.score(X_test, y_test)  # evaluate performance\n",
    "```\n",
    "The regressor provides an implementation of `.score`.  Conforming to this convention has the benefit that many tools (e.g. cross-validation, grid search) rely on this interface so you can use your new estimators with the existing `sklearn` infrastructure.\n",
    "\n",
    "For example `grid_search.GridSearchCV` ([docs](http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.GridSearchCV.html)) takes an estimator and some hyperparameters as arguments, and returns another estimator.  Upon fitting, it fits the best model (based on the inputted hyperparameters) and uses that for prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. CityModel\n",
    "\n",
    "`sklearn` provides a number of different types of estimators, which we will use for subsequent problems. For this first problem, we will write our own (simple) estimator.\n",
    "\n",
    "This estimator will look only at one piece of data: which city the venue is in. You can image that the ratings in some cities are probably higher than others on average.\n",
    "\n",
    "When the `fit()` function is called, the estimator will use `groupby` and `mean` to compute the average rating for each city and store that information within the class. Note that in order for this to work, it will be necessary to join the `X` and `y` dataframes. This can easily be done by using:\n",
    "\n",
    "```python\n",
    "joined_df = X.join(y)\n",
    "```\n",
    "\n",
    "When a new observation is requested from the `predict()` function, the estimator should retrieve and report the average rating belonging to the city of the new observation. If the city of the new observation was not in the training set, return instead the average of all venues.\n",
    "\n",
    "**Note:** It may be tempting to create an estimator which requires that you give only the name of the city as its input. Don't do it... it will make your life harder later. For every model in this project, build estimators which accept the full dataset as inputs. In other words, design your estimator so that you can use it as follows:\n",
    "```python\n",
    "y_pred = city_model.predict(X_test)\n",
    "```\n",
    "where `X_test` is the result of your call to `train_test_split` above. Make sure the type of `y_pred` is `ndarray`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CityModel\n",
    "\n",
    "class Estimator(sk.base.BaseEstimator, sk.base.RegressorMixin):\n",
    "    \n",
    "    def __init__(self,groupby_col = 'city',mean_col = 'stars'):\n",
    "        self.groupby_col = groupby_col\n",
    "        self.mean_col = mean_col\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        joined_df = X.join(y)\n",
    "        self.fit_df = joined_df.groupby([self.groupby_col])[self.mean_col].mean().to_frame()\n",
    "        return self\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        stars = []\n",
    "        for index, row in X.iterrows():\n",
    "            try:\n",
    "                stars.append(self.fit_df.loc[row[self.groupby_col]][self.mean_col])\n",
    "            except:\n",
    "                stars.append(self.fit_df[self.mean_col].mean())\n",
    "        return np.array(stars)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_estimator =  Estimator()\n",
    "my_estimator.fit(X_train,y_train)\n",
    "my_estimator.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformers\n",
    "\n",
    "In the previous problem, we created the estimator and were able to design it in such a way that it accepted input in the format of our choosing.\n",
    "\n",
    "For more complex operations, we will prefer to use estimators provided to us by `sklearn`. To do so, we sometimes need to process or **transform** the data before we can do machine-learning on it.  `sklearn` has Transformers to help with this.  They implement this interface:\n",
    "``` python\n",
    "class Transformer(base.BaseEstimator, base.TransformerMixin):\n",
    "  def __init__(self, ...):\n",
    "   # initialization code\n",
    "   return\n",
    "\n",
    "  def fit(self, X, y=None):\n",
    "    # fit the transformation ... (often this is empty!)\n",
    "    return self\n",
    "\n",
    "  def transform(self, X):\n",
    "    return ... # transformation\n",
    "```\n",
    "When combined with our previous `estimator`, the intended usage is\n",
    "``` python\n",
    "transformer = Transformer(...)  # initialize\n",
    "X_trans_train = transformer.fit_transform(X_train)  # fit / transform data\n",
    "estimator.fit(X_trans_train, y_train)  # fit new model on training data\n",
    "X_trans_test = transformer.transform(X_test)  # transform test data\n",
    "estimator.score(X_trans_test, y_test)  # fit new model\n",
    "```\n",
    "Here, `.fit_transform` is implemented based on the `.fit` and `.transform` methods in `base.TransformerMixin` ([docs](http://scikit-learn.org/stable/modules/generated/sklearn.base.TransformerMixin.html)).  Especially for transformers, `.fit` is often empty and only `.transform` actually does something."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipelines\n",
    "One of the main advantages of using sklearn transformers is that we can chain them together with pipelines.  For example, this\n",
    "``` python\n",
    "new_model = pipeline.Pipeline([\n",
    "    ('trans', Transformer(...)),     # Add a transformer to the pipeline\n",
    "    ('est', Estimator(...))          # Add an estimator to the pipeline\n",
    "  ])\n",
    "new_model.fit(X_train, y_train)\n",
    "new_model.score(X_test, y_test)\n",
    "```\n",
    "would replace all the fitting and scoring code above.  The pipeline itself is an estimator (and implements the `.fit` and `.predict` methods).  Note that a pipeline can have multiple transformers chained up but at most one (optional) terminal estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. LatLongModel\n",
    "\n",
    "We will now build a more fine-grained model based on geographical location.  We know that some neighborhoods are trendier than others.  To account for this, we might consider a K Nearest Neighbors or Random Forest based on the latitude and longitude.\n",
    "\n",
    "This time, we will use a built-in estimator for `sklearn`. However, in order that the result of our work is reuasable later when we build our FullModel, we want to have an estimator which accepts the entire dataset as its input. For this reason, we will create a custom transformer which accepts our data in its original format and transforms it into something suitable for the built-in `sklearn` estimators (i.e., `numpy.ndarray`). By putting this transformer and our selected estimator into a pipeline together, we effectively create a single estimator which both accepts the data in our original format and takes advantage of `sklearn`'s algorithms.\n",
    "\n",
    "Let's focus on the transformer first. Implement a generic `ColumnSelectTransformer` that is passed which columns to select when initialized. **Hint:** You will want to include some code in the transformer's `__init__()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# LatLong model\n",
    "\n",
    "class ColumnSelectTransformer(sk.base.BaseEstimator, sk.base.TransformerMixin):\n",
    "    def __init__(self,column_list):\n",
    "        if type(column_list) == list:\n",
    "            self.column_list = column_list\n",
    "        else:\n",
    "            raise Exception(\"Param must be list\")\n",
    "        return\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        if set(self.column_list).issubset(X.columns):\n",
    "            return X[self.column_list]\n",
    "        else:\n",
    "            raise Exception(\"Some columns may not exist: {}\".format(self.column_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use `neighbors.KNeighborsRegressor` ([docs](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html)) as the estimator. Create a pipeline which contains the transformer you just created as well as one of these estimators. For now, we'll set the hyperparameters `n_neighbors` to 5. In the next step, we'll find the optimal value for this hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline with KNeighborsRegressor\n",
    "from sklearn import pipeline\n",
    "from sklearn import neighbors\n",
    "\n",
    "k_pipe = pipeline.Pipeline([\n",
    "  ('truncate', ColumnSelectTransformer([\"latitude\",\"longitude\"])),\n",
    "  ('knearestRegressor', neighbors.KNeighborsRegressor(n_neighbors=119))\n",
    "  ])\n",
    "k_pipe.fit(X_train, y_train)\n",
    "print(k_pipe.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now find the optimal value for the hyperparameter `n_neighbors`. \n",
    "\n",
    "Pass your pipeline to `grid_search.GridSearchCV` ([doc](http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.GridSearchCV.html)). You'll need to specify a dictionary for the parameter `param_grid`. Be sure to experiment the number of parameters to find the optimal value. After calling the `fit()` function on your `GridSearchCV` object, you can find the estimator with the best hyperparameter in the `.best_estimator_` parameter. Store this estimator for the full model later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search for hyperparameter\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "n_neighbors = np.arange(1,200)\n",
    "d = dict()\n",
    "d['knearestRegressor__n_neighbors']=n_neighbors\n",
    "gscv_neighbors = GridSearchCV(estimator=k_pipe, param_grid=d, cv= 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gscv_neighbors.fit(X_train, y_train)\n",
    "print(gscv_neighbors.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. CategoryModel\n",
    "\n",
    "Venues have categories with varying degrees of specificity, e.g.\n",
    "\n",
    "``` python\n",
    "  [Doctors, Health & Medical]\n",
    "  [Restaurants]\n",
    "  [American (Traditional), Restaurants]\n",
    "```\n",
    "  \n",
    "With a large sparse feature set like this, we often use a cross-validated regularized linear model.\n",
    "\n",
    "Build a custom transformer that massages the data so that it can be fed into `feature_extraction.DictVectorizer` ([docs](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.DictVectorizer.html),[Example](Dict Vectorizer Example.ipynb)), which in turn generates a large matrix gotten by One-Hot-Encoding. \n",
    "\n",
    "Use a pipeline to feed the result of this into a `LinearRegression` estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CategoryModel\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MultipleCategoryTransformer(sk.base.BaseEstimator, sk.base.TransformerMixin):\n",
    "    def __init__(self,dict_vect,category_column_name = \"categories\"):\n",
    "        self.category_column_name = category_column_name\n",
    "        self.dict_vect = dict_vect\n",
    "        return\n",
    "\n",
    "    def multiple_transform(self,X,category_column_name):\n",
    "        column_categories = []\n",
    "        for index, row in X.iterrows():\n",
    "            row_categories = {}\n",
    "            for category in row[category_column_name]:\n",
    "                row_categories[category] = True\n",
    "            column_categories.append(row_categories)\n",
    "        return column_categories\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        self.dict_vect.fit(self.multiple_transform(X,self.category_column_name))\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return self.dict_vect.transform(self.multiple_transform(X,self.category_column_name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn import pipeline\n",
    "\n",
    "v = DictVectorizer(sparse=False)\n",
    "m = MultipleCategoryTransformer(v)\n",
    "r_pipe = pipeline.Pipeline([\n",
    "  ('truncate', m),\n",
    "  ('linearregression', linear_model.LinearRegression())\n",
    "  ])\n",
    "\n",
    "\n",
    "r_pipe.fit(X_train, y_train)\n",
    "print(r_pipe.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. AttributeModel\n",
    "\n",
    "Venues have (potentially nested) attributes.  For example,\n",
    "\n",
    "``` python\n",
    "  { 'Attire': 'casual',\n",
    "    'Accepts Credit Cards': True,\n",
    "    'Ambience': {'casual': False, 'classy': False }}\n",
    "```\n",
    "  \n",
    "Categorical data like this should often be transformed by a One Hot Encoding.  For example, we might flatten the above into something like this:\n",
    "\n",
    "``` python\n",
    "  { 'Attire_casual' : 1,\n",
    "    'Accepts Credit Cards': 1,\n",
    "    'Ambience_casual': 0,\n",
    "    'Ambience_classy': 0 }\n",
    "```\n",
    "\n",
    "Build a custom transformer that flattens attributes and feed this into `DictVectorizer`.  Feed it into a (cross-validated) linear model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# AttributeModel\n",
    "print(X_train[\"attributes\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultipleCategoryTransformer2(sk.base.BaseEstimator, sk.base.TransformerMixin):\n",
    "    def __init__(self,dict_vect,category_column_name = \"attributes\"):\n",
    "        self.category_column_name = category_column_name\n",
    "        self.dict_vect = dict_vect\n",
    "        return\n",
    "    \n",
    "    def unpack(self,d, parent_key='', sep='_'):\n",
    "        items = []\n",
    "        for k, v in d.items():\n",
    "            new_key = parent_key + sep + k if parent_key else k\n",
    "            if isinstance(v, dict):\n",
    "                items.extend(self.unpack(v, new_key, sep=sep).items())\n",
    "            else:\n",
    "                if isinstance(v,bool):\n",
    "                    if (v):\n",
    "                        items.append((new_key, 1))\n",
    "                    else:\n",
    "                        items.append((new_key, 0))\n",
    "                elif isinstance(v,int):\n",
    "                    items.append((new_key+ sep + str(v), 1))\n",
    "                elif isinstance(v,str):\n",
    "                    items.append((new_key+ sep + v, 1))\n",
    "                else:\n",
    "                    print(type(v))\n",
    "                    print(v)\n",
    "        return dict(items)\n",
    "    \n",
    "    def unpacking_columns(self,X,category_column_name):\n",
    "        column_categories = []\n",
    "        for index, row in X.iterrows():\n",
    "            column_categories.append(self.unpack(row[category_column_name]))\n",
    "        return column_categories\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        self.dict_vect.fit(self.unpacking_columns(X,self.category_column_name))\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return self.dict_vect.transform(self.unpacking_columns(X,self.category_column_name))\n",
    "    \n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn import pipeline\n",
    "\n",
    "v = DictVectorizer(sparse=False)\n",
    "m = MultipleCategoryTransformer2(v)\n",
    "l_pipe = pipeline.Pipeline([\n",
    "  ('truncate', m),\n",
    "  ('lassocv', linear_model.LassoCV())\n",
    "  ])\n",
    "\n",
    "\n",
    "l_pipe.fit(X_train, y_train)\n",
    "print(l_pipe.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## V. FullModel\n",
    "\n",
    "So far we have only built models based on individual features.  Now, we combine them into An **ensemble model**.  We do this using a [feature union](http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.FeatureUnion.html).\n",
    "\n",
    "Combine all the above models using a feature union.  Notice that a feature union takes transformers, not models as arguments.  The way around this is to convert your existing estimators into transformers. You can do this manually by modifying the estimators you've written so far, but an easier way is to wrap your estimators in a class such as [this](Model Transformer.ipynb).\n",
    "\n",
    "The feature union itself is a transformer. Use a pipeline to combine this transformer with linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FullModel\n",
    "\n",
    "class ModelTransformer(sk.base.TransformerMixin):\n",
    "\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def fit(self, *args, **kwargs):\n",
    "        self.model.fit(*args, **kwargs)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, **transform_params):\n",
    "        return pd.DataFrame(self.model.predict(X))\n",
    "    \n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn import pipeline\n",
    "\n",
    "#City\n",
    "my_estimator =  Estimator()\n",
    "\n",
    "#Lang Long\n",
    "k_pipe = pipeline.Pipeline([\n",
    "  ('truncate', ColumnSelectTransformer([\"latitude\",\"longitude\"])),\n",
    "  ('knearestRegressor', neighbors.KNeighborsRegressor(n_neighbors=119))\n",
    "  ])\n",
    "\n",
    "#Categories\n",
    "v = DictVectorizer(sparse=False)\n",
    "m = MultipleCategoryTransformer(v)\n",
    "r_pipe = pipeline.Pipeline([\n",
    "  ('truncate', m),\n",
    "  ('linearregression', linear_model.LinearRegression())\n",
    "  ])\n",
    "\n",
    "#Attributes\n",
    "v1 = DictVectorizer(sparse=False)\n",
    "m1 = MultipleCategoryTransformer2(v1)\n",
    "l_pipe = pipeline.Pipeline([\n",
    "  ('truncate', m1),\n",
    "  ('lassocv', linear_model.LassoCV())\n",
    "  ])\n",
    "\n",
    "pipe = pipeline.Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "        ('city',  ModelTransformer(my_estimator)),\n",
    "        ('longlat', ModelTransformer(k_pipe)),\n",
    "        ('categories', ModelTransformer(r_pipe)),\n",
    "        ('attributes', ModelTransformer(l_pipe))\n",
    "        ])),\n",
    "    ('finalestimator', linear_model.LinearRegression())\n",
    "    ])\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "print(pipe.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
